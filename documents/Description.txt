Task: Colorization

Model Archtiecture:
The model architecture consists of three main components: a U-Net generator (net_G), a patch-based discriminator (net_D), and various loss functions and optimization algorithms.

The U-Net generator is implemented as a U-Net, which is a widely used architecture for image-to-image translation tasks. It takes a grayscale input image (L channel) and aims to generate a colored image (ab channels). The U-Net consists of an encoder-decoder structure with skip connections. The number of downsampling and upsampling layers can be controlled through the parameters n_down and num_filters, respectively. Each U-Net block (UnetBlock) performs convolution, normalization, and activation operations. The innermost and outermost blocks have specific configurations, while the intermediate blocks include an optional dropout layer.

The patch-based discriminator is designed to distinguish between real and fake images. It focuses on local image patches rather than the entire image. The architecture consists of multiple convolutional layers with increasing numbers of filters and decreasing spatial dimensions. Each layer is followed by batch normalization and leaky ReLU activation, except for the last layer, which has no activation or normalization. The number of downsampling layers can be controlled through the parameter n_down.

The model also includes the GANLoss class, which provides different types of loss functions for the generator and discriminator. The available options are vanilla GAN loss (BCEWithLogitsLoss) and least squares GAN loss (MSELoss). The MainModel class initializes the generator, discriminator, loss functions, and optimizers. It defines methods for setting input data, performing forward and backward passes, and optimizing the networks. The model is trained using the Adam optimizer with adjustable learning rates (lr_G and lr_D) and momentum terms (beta1 and beta2). Additionally, an L1 loss is used to encourage pixel-level similarity between the generated and target images, weighted by the lambda_L1 parameter.

Overall, the architecture follows a typical GAN setup for image colorization. The U-Net generator aims to produce realistic colorized images, while the patch-based discriminator learns to distinguish between real and fake images. The model is trained using a combination of GAN loss and L1 loss, optimized with the Adam optimizer.

Training:

The model was trained using a combination of adversarial and pixel-level similarity losses in a GAN (Generative Adversarial Network) setup for image colorization. The training process involved iterating through the training dataset for a specified number of epochs.
During each epoch, the model processed batches of training data and performed optimization steps. The loss values were tracked using the AverageMeter class, which maintained running averages of different loss components such as generator (G) and discriminator (D) losses.
The train_model function controlled the training loop. It updated the model's input data, performed optimization steps, and updated the loss meters. At specific intervals defined by the display_every parameter, the function printed the current epoch, iteration, and the average losses. Additionally, the visualize function was called to display the model's output images for a batch of data.
The training process aimed to minimize the generator loss (loss_G) to a threshold (min_g_loss). If the generator loss fell below the threshold, the model's state was saved as a checkpoint. This allowed for tracking the best-performing model during training.
After the training process, the final model state was saved as "GANModels/colorization_model.pth" using the torch.save() function. This saved model can be later used for colorizing grayscale images using the trained generator network.

